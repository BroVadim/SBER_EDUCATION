{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462ef961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HH_vacancies:\n",
    "    def __init__(self, ):\n",
    "        res_df  = self.search_areas_ids()\n",
    "        self.areas_ids = res_df['id_y'].unique().tolist()\n",
    "        \n",
    "    def search_areas_ids(self):\n",
    "        #строка запроса, где 113 - код России\n",
    "        url = 'https://api.hh.ru/areas/113'\n",
    "        # отправляю запрос и полученный ответ преобразовываю в json\n",
    "        result = requests.get(url).json()['areas']\n",
    "        # преобразовываю в DataFrame для удобства дальнейшей работы с данными\n",
    "        df = pd.json_normalize(result)\n",
    "        # произвожу поиск вложенных areas для parent_ids\n",
    "        areas_list = []\n",
    "        for item in df['areas'].tolist():\n",
    "            areas_list += item\n",
    "        df2 = pd.json_normalize(areas_list)\n",
    "        res_df = df.drop('areas', axis=1).merge(\n",
    "                                                df2.drop('areas', axis=1),\n",
    "                                                left_on='id',\n",
    "                                                right_on='parent_id',\n",
    "                                                how='inner'\n",
    "                                            )\n",
    "        # удаляю колонки, дублирующие данные\n",
    "        res_df = res_df.drop(columns = ['parent_id_x','parent_id_y'], axis=1)\n",
    "        return res_df\n",
    "\n",
    "    def search_vacancies_ids(self, start, stop, filename):\n",
    "        del_columns = [\n",
    "            'address', 'response_url', 'sort_point_distance', 'relations',\n",
    "            'contacts', 'working_days', 'working_time_intervals', 'working_time_modes',\n",
    "            'accept_temporary', 'address.building', 'address.description',\n",
    "            'address.metro', 'address.metro_stations', 'employer.logo_urls.240',\n",
    "            'employer.logo_urls.90', 'employer.logo_urls.original', 'address.metro.station_name',\n",
    "            'address.metro.line_name', 'address.metro.station_id', 'address.metro.line_id',\n",
    "            'address.metro.lat', 'address.metro.lng', 'salary', 'insider_interview.id',\n",
    "            'insider_interview.url'\n",
    "        ]\n",
    "        res_frame = pd.DataFrame()\n",
    "        for area_id in tqdm(self.areas_ids[start:stop]):\n",
    "            try:\n",
    "                for page in range(1,100):\n",
    "                    url = f'https://api.hh.ru/vacancies?page={page}&per_page=100&area={area_id}'\n",
    "                    time.sleep(0.5)\n",
    "                    req = requests.get(url).json()\n",
    "                    new_frame = pd.json_normalize(req['items'])\n",
    "                    res_frame = pd.concat([res_frame, new_frame])\n",
    "                    if len(req['items'])==0:\n",
    "                        break\n",
    "            except Exception as error:\n",
    "                continue\n",
    "        res_frame = res_frame.drop(columns = del_columns)\n",
    "        res_frame.to_csv(filename, sep=';', index = False)\n",
    "    \n",
    "    def get_vacancies_data(self, vacancies_ids, left, right):\n",
    "        res_df = pd.DataFrame()\n",
    "        count = 0\n",
    "        for vacancy_id in tqdm(vacancies_ids[left : right]):\n",
    "            try:\n",
    "                url = f'https://api.hh.ru/vacancies/{vacancy_id}'\n",
    "                res = requests.get(url)\n",
    "                if res.status_code == 200 and len(res.text)>10:\n",
    "                    df = pd.json_normalize(res.json())\n",
    "                    res_df = pd.concat([res_df, df])\n",
    "                    count += 1\n",
    "                    if count == 100:\n",
    "                        time.sleep(5)\n",
    "                        count = 0\n",
    "                else:\n",
    "                    time.sleep(2)\n",
    "                    raise Exception('Not 200 status')\n",
    "            except Exception as error:\n",
    "                continue\n",
    "        res_df.to_excel(f'{left}-{right}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0842cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh = HH_vacancies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237eae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = [\n",
    "    [600, 650],[650, 700],[700, 750]\n",
    "]\n",
    "for limit in limits:\n",
    "    hh.search_vacancies_ids(limit[0], limit[1], f'areas_{limit[0]}_{limit[1]}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_df = pd.read_csv('Result_vacancies_ids.csv', sep=';')\n",
    "ids = ids_df['id'].tolist()\n",
    "hh.get_vacancies_data(ids, 25000, 26000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dec4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923e02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212cc46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd33e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_files = [file for file in os.listdir() if file[-4:]=='xlsx']\n",
    "columns = [\n",
    "    'id','name','description','key_skills','specializations','professional_roles',\n",
    "    'published_at','apply_alternate_url','alternate_url','area.id','area.name',\n",
    "    'salary.from','salary.to','salary.currency','experience.id','experience.name',\n",
    "    'schedule.name','employment.name','employer.id','employer.name','address.city',\n",
    "    'address.street','address.building','address.lat','address.lng','address.raw'\n",
    "]\n",
    "res_df = pd.DataFrame()\n",
    "for excel_file in excel_files:\n",
    "    try:\n",
    "        df = pd.read_excel(excel_file)[columns]\n",
    "        res_df = pd.concat([res_df, df])\n",
    "    except Exception as error:\n",
    "        print(f'{error}-{excel_file}')\n",
    "res_df.to_excel('Result_vac.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f3446c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a793508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Similarity:\n",
    "    def __init__(self):\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        additional_stopwords = ['quot']\n",
    "        self.del_words = stopwords.words('russian') + additional_stopwords\n",
    "    \n",
    "    def del_tags(self, text):\n",
    "        \"\"\"\n",
    "        Метод для очистки входного текста от html-тегов.\n",
    "        input:  text - текст для очистки.\n",
    "        output: очищенная строка.\n",
    "        \"\"\"\n",
    "        return re.sub('\\<.{1,9}\\>',' ',text)\n",
    "    \n",
    "    def clean_text(self, dirty_text):\n",
    "        \"\"\"\n",
    "        Метод для очистки текста.\n",
    "        input:  dirty_text - текст для очистки.\n",
    "        output: result_text - токенизированный текст без стоп-слов и в нормальной форме.\n",
    "        \"\"\"\n",
    "        sub_text = re.sub('[^А-Яа-яA-Za-z\\s\\|]|\\s{2,}',' ', self.del_tags(dirty_text) ).strip().lower()\n",
    "        tokens = word_tokenize(str(sub_text))\n",
    "        clean_tokens = tuple(map(lambda x: self.morph.parse(x)[0].normal_form if x not in self.del_words else '', tokens ))\n",
    "        result_text = ' '.join(clean_tokens)\n",
    "        return result_text\n",
    "    \n",
    "    def vectorize_texts(self, texts_for_vectorized, minimal_df):\n",
    "        \"\"\"\n",
    "        Метод для векторизации списка текстов.\n",
    "        input:  texts_for_vectorized - список текстов для векторизации.\n",
    "        output: vectors_list - список векторов на основе представленных текстов.\n",
    "        \"\"\"\n",
    "        vectorizer = CountVectorizer(min_df = minimal_df)\n",
    "        vectorizer.fit(texts_for_vectorized)\n",
    "        print(f'Количество слов в словаре {vectorizer.vocabulary_}')\n",
    "        x = vectorizer.transform(texts_for_vectorized)\n",
    "        vectors = x.toarray()\n",
    "        vectors_list = [vector for vector in vectors]\n",
    "        return vectors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = Similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3231896",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Result_vac.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d141a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69082ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df\n",
    "new_df['description_2'] = new_df['description'].apply(lambda x: sim.clean_text(str(x)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65774c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_excel('Test2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a01d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
