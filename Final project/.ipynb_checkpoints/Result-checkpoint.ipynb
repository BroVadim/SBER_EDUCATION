{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pymorphy2\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import re\n",
    "import telebot\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c0b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HH_vacancies:\n",
    "    def __init__(self, ):\n",
    "        res_df  = self.search_areas_ids()\n",
    "        self.areas_ids = res_df['id_y'].unique().tolist()\n",
    "        \n",
    "    def search_areas_ids(self):\n",
    "        #строка запроса, где 113 - код России\n",
    "        url = 'https://api.hh.ru/areas/113'\n",
    "        # отправляю запрос и полученный ответ преобразовываю в json\n",
    "        result = requests.get(url).json()['areas']\n",
    "        # преобразовываю в DataFrame для удобства дальнейшей работы с данными\n",
    "        df = pd.json_normalize(result)\n",
    "        # произвожу поиск вложенных areas для parent_ids\n",
    "        areas_list = []\n",
    "        for item in df['areas'].tolist():\n",
    "            areas_list += item\n",
    "        df2 = pd.json_normalize(areas_list)\n",
    "        res_df = df.drop('areas', axis=1).merge(\n",
    "                                                df2.drop('areas', axis=1),\n",
    "                                                left_on='id',\n",
    "                                                right_on='parent_id',\n",
    "                                                how='inner'\n",
    "                                            )\n",
    "        # удаляю колонки, дублирующие данные\n",
    "        res_df = res_df.drop(columns = ['parent_id_x','parent_id_y'], axis=1)\n",
    "        return res_df\n",
    "\n",
    "    def search_vacancies_ids(self, start, stop, filename):\n",
    "        del_columns = [\n",
    "            'address', 'response_url', 'sort_point_distance', 'relations',\n",
    "            'contacts', 'working_days', 'working_time_intervals', 'working_time_modes',\n",
    "            'accept_temporary', 'address.building', 'address.description',\n",
    "            'address.metro', 'address.metro_stations', 'employer.logo_urls.240',\n",
    "            'employer.logo_urls.90', 'employer.logo_urls.original', 'address.metro.station_name',\n",
    "            'address.metro.line_name', 'address.metro.station_id', 'address.metro.line_id',\n",
    "            'address.metro.lat', 'address.metro.lng', 'salary', 'insider_interview.id',\n",
    "            'insider_interview.url'\n",
    "        ]\n",
    "        res_frame = pd.DataFrame()\n",
    "        for area_id in tqdm(self.areas_ids[start:stop]):\n",
    "            try:\n",
    "                for page in range(1,100):\n",
    "                    url = f'https://api.hh.ru/vacancies?page={page}&per_page=100&area={area_id}'\n",
    "                    time.sleep(0.5)\n",
    "                    req = requests.get(url).json()\n",
    "                    new_frame = pd.json_normalize(req['items'])\n",
    "                    res_frame = pd.concat([res_frame, new_frame])\n",
    "                    if len(req['items'])==0:\n",
    "                        break\n",
    "            except Exception as error:\n",
    "                continue\n",
    "        res_frame = res_frame.drop(columns = del_columns)\n",
    "        res_frame.to_csv(filename, sep=';', index = False)\n",
    "    \n",
    "    def get_vacancies_data(self, vacancies_ids, left, right):\n",
    "        res_df = pd.DataFrame()\n",
    "        count = 0\n",
    "        columns = [\n",
    "            'id','name','description','key_skills','specializations','professional_roles',\n",
    "            'published_at','apply_alternate_url','alternate_url','area.id','area.name',\n",
    "            'salary.from','salary.to','salary.currency','experience.id','experience.name',\n",
    "            'schedule.name','employment.name','employer.id','employer.name','address.city',\n",
    "            'address.street','address.building','address.lat','address.lng','address.raw']\n",
    "        for vacancy_id in tqdm(vacancies_ids[left : right]):\n",
    "            try:\n",
    "                url = f'https://api.hh.ru/vacancies/{vacancy_id}'\n",
    "                res = requests.get(url)\n",
    "                if res.status_code == 200 and len(res.text)>10:\n",
    "                    df = pd.json_normalize(res.json())\n",
    "                    res_df = pd.concat([res_df, df[columns]])\n",
    "                    count += 1\n",
    "                    if count == 100:\n",
    "                        time.sleep(5)\n",
    "                        count = 0\n",
    "                else:\n",
    "                    time.sleep(2)\n",
    "                    raise Exception('Not 200 status')\n",
    "            except Exception as error:\n",
    "                continue\n",
    "        res_df.to_excel(f'{left}-{right}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6427f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_worker:\n",
    "    def __init__(self):\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        additional_stopwords = ['quot']\n",
    "        self.del_words = stopwords.words('russian') + additional_stopwords\n",
    "    \n",
    "    def del_tags(self, text):\n",
    "        \"\"\"\n",
    "        Метод для очистки входного текста от html-тегов.\n",
    "        input:  text - текст для очистки.\n",
    "        output: очищенная строка.\n",
    "        \"\"\"\n",
    "        return re.sub('\\<.{1,9}\\>',' ',text)\n",
    "    \n",
    "    def clean_text(self, dirty_text):\n",
    "        \"\"\"\n",
    "        Метод для очистки текста.\n",
    "        input:  dirty_text - текст для очистки.\n",
    "        output: result_text - токенизированный текст без стоп-слов и в нормальной форме.\n",
    "        \"\"\"\n",
    "        sub_text = re.sub('[^А-Яа-яA-Za-z\\s\\|]|\\s{2,}',' ', self.del_tags(dirty_text) )\n",
    "        n_sub_text = re.sub('\\s{2,10}',' ', sub_text).strip().lower()\n",
    "        tokens = word_tokenize(str(n_sub_text))\n",
    "        clean_tokens = tuple(map(lambda x: self.morph.parse(x)[0].normal_form if x not in self.del_words else '', tokens ))\n",
    "        result_text = ' '.join(clean_tokens)\n",
    "        return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = telebot.TeleBot('5689764563:AAHuZaNiVga_NBwlst2EELS46Qevxe_QOoY')\n",
    "\n",
    "text_worker = Text_worker()\n",
    "url = 'https://drive.google.com/uc?export=download&confirm=no_antivirus&id=1rNdkRAa8ilISD2mrrveDJSeV5iz4f8hW'\n",
    "df = pd.read_csv(url, sep='|')\n",
    "df['description_2'] = df['description_2'].apply(lambda x: re.sub(r'\\s{2,10}',' ',str(x)) )\n",
    "docs = df['description_2'].astype(str).tolist()\n",
    "card_docs = [TaggedDocument(doc.split(' '), [i]) for i, doc in enumerate(docs)]\n",
    "model = Doc2Vec(vector_size=64, window=2, min_count=10, workers=8, epochs = 40)\n",
    "model.build_vocab(card_docs)\n",
    "model.train(card_docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "print('Модель натренирована')\n",
    "# Функция, обрабатывающая команду /start\n",
    "@bot.message_handler(commands=[\"start\"])\n",
    "def start(m, res=False):\n",
    "    greeting = \"\"\"\n",
    "    Здравствуйте.\n",
    "    Для продолжения работы отправьте сообщение с текстовым описанием вакансии для поиска.\n",
    "    \"\"\"\n",
    "    bot.send_message(m.chat.id, greeting)\n",
    "\n",
    "# Получение сообщений от юзера\n",
    "@bot.message_handler(content_types=[\"text\"])\n",
    "def handle_text(message):\n",
    "    try:\n",
    "        splited_text = re.sub('\\s{2,5}',' ', text_worker.clean_text( str(message)) ).split(' ')\n",
    "        vector_to_search = model.infer_vector(splited_text)\n",
    "        sim_documents = model.docvecs.most_similar([vector_to_search], topn=5)\n",
    "        \n",
    "        urls = [str(df['alternate_url'].iloc[s[0]]) for s in sim_documents]\n",
    "        urls_string = '\\n'.join(urls)\n",
    "        res_message = f'Ссылки на наиболее похожие вакансии:\\n{urls_string}'\n",
    "        bot.send_message(message.chat.id, res_message)\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        bot.send_message(message.chat.id, 'При попытке запроса произошла ошибка, повторите попытку...')\n",
    "\n",
    "bot.polling(none_stop=True, interval=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b4aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rezume = \"\"\"АО НИПИГАЗ\n",
    "Эксперт СМР и КИП\n",
    "Организация строительства АГПЗ\n",
    "Организация строительного контроля в структуре заказчика;\n",
    "Составление графиков выполнения работ и контроль их реализации;\n",
    "Оперативное планирование выполнения СМР подрядными организациями;\n",
    "Проведение входного контроля проектной и рабочей документации;\n",
    "Проверка факта выполненных объемов работ;\n",
    "Проверка и подписание исполнительной документации;\n",
    "Визирование актов выполненных работ;\n",
    "Участие в проведении входного контроля МТР;\n",
    "Контроль исполнения договорных обязательств подрядными организациями;\n",
    "Ведения деловой переписки с подрядными организациями в части исполнения обязательств по строительству и контролю выполненных работ\n",
    "Взаимодействия с подрядными организациями в части организации строительного производства;\n",
    "Координации подрядных организаций в вопросах связанных с исполнением договорных обязательств;\n",
    "Участие в совещаниях с подрядными организациями по вопросам, связанным со строительным производством;\n",
    "Имею высшее техническое образование \"Управление и информатика в технологических системах\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56246fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
