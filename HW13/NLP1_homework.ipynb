{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH7qx_irU4Y8"
   },
   "source": [
    "###ML1_1: \n",
    "https://www.hackerrank.com/challenges/capturing-non-capturing-groups/problem?isFullScreen=true\n",
    "\n",
    "###ML1_2: \n",
    "https://www.hackerrank.com/challenges/branch-reset-groups/problem?isFullScreen=true\n",
    "\n",
    "###ML1_3: \n",
    "https://www.hackerrank.com/challenges/detect-html-links/problem?isFullScreen=true\n",
    "\n",
    "###ML1_4: Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\darmorezov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\darmorezov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pymorphy2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "You have a test String .\n",
    "Your task is to write a regex which will match  with the following condition:\n",
    "-should have  or more consecutive repetitions of ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_ok_repeats(string):\n",
    "    result = re.findall(r'(ok){3,}', string)\n",
    "    return True if len(result)>0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test string: asdfgfdasdfdgokokokqwewetwt, Result: True\n",
      "Test string: 2134sdfsdgy54daok, Result: False\n",
      "Test string: 3tsdgdfhsacvgnyrh wgdfv wdfsgd, Result: False\n",
      "Test string: ok, Result: False\n",
      "Test string: okasfdsfokok, Result: False\n"
     ]
    }
   ],
   "source": [
    "test_strings = [\n",
    "    'asdfgfdasdfdgokokokqwewetwt',\n",
    "    '2134sdfsdgy54daok',\n",
    "    '3tsdgdfhsacvgnyrh wgdfv wdfsgd',\n",
    "    'ok',\n",
    "    'okasfdsfokok'\n",
    "]\n",
    "for test_string in test_strings:\n",
    "    print(f'Test string: {test_string}, Result: {search_ok_repeats(test_string)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJfkstKpqsXp"
   },
   "source": [
    "# Task 2\n",
    "\n",
    "You have a test string .\n",
    "Your task is to write a regex which will match , with following condition(s):\n",
    "\n",
    "-consists of 8 digits.\n",
    "-must have \"---\", \"-\", \".\" or \":\" separator such that string  gets divided in  parts, with each part having exactly two digits.\n",
    "-string must have exactly one kind of separator.\n",
    "Separators must have integers on both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_separators(string):\n",
    "    patterns = [\n",
    "       r'\\d{2}-\\d{2}-\\d{2}-\\d{2}', \n",
    "        r'\\d{2}-{3}\\d{2}-{3}\\d{2}-{3}\\d{2}',\n",
    "        r'\\d{2}\\.\\d{2}\\.\\d{2}\\.\\d{2}',\n",
    "        '\\d{2}:{1}\\d{2}:{1}\\d{2}:{1}\\d{2}'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, string) != None:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test string: 213--45-123-3, Result: False\n",
      "Test string: 22::44:12:45, Result: False\n",
      "Test string: 12:34:56:78, Result: True\n",
      "Test string: 12-34-56-78, Result: True\n",
      "Test string: 12---34--56---78, Result: False\n",
      "Test string: sa:45:bt:56, Result: False\n",
      "Test string: 45..34..56..12, Result: False\n"
     ]
    }
   ],
   "source": [
    "test_strings = [\n",
    "    '213--45-123-3',\n",
    "    '22::44:12:45',\n",
    "    '12:34:56:78',\n",
    "    '12-34-56-78',\n",
    "    '12---34--56---78',\n",
    "    'sa:45:bt:56',\n",
    "    '45..34..56..12'\n",
    "]\n",
    "for test_string in test_strings:\n",
    "    print(f'Test string: {test_string}, Result: {search_separators(test_string)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "Input Format\n",
    "\n",
    "The first line contains the number of lines in the fragment (N). This is followed by N lines from a valid HTML document or fragment.\n",
    "\n",
    "Constraints\n",
    "\n",
    "N < 100\n",
    "Number of characters in the test fragments <= 10000 characters.\n",
    "Characters will be restricted to ASCII. Fragments for the tests will be picked up from Wikipedia. Also, some tests might not have text or names on the links.\n",
    "\n",
    "Output Format\n",
    "\n",
    "If there are M links in the document, display each of them in a new line. The link and the text name must be separated by a \",\" (comma) with no spaces between them.\n",
    "Strip out any extra spaces at the start and end position of both the link and the text name before printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_links(string):\n",
    "    lines = string.split('\\n')\n",
    "    result_list = []\n",
    "    for line in lines[1:]:\n",
    "        try:\n",
    "            link = re.findall(r'href=\\\"(\\S+)\"', line)[-1]\n",
    "            text = re.findall(r'<a .*>(.*)<\\/a>', line)[-1]\n",
    "            result_list.append(f'{link},{text}')\n",
    "        except:\n",
    "            continue\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text 1\n",
      "/wiki/Main_Page,Main page\n",
      "/wiki/Portal:Contents,Contents\n",
      "/wiki/Portal:Featured_content,Featured content\n",
      "/wiki/Portal:Current_events,Current events\n",
      "/wiki/Special:Random,Random article\n",
      "//donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en,Donate to Wikipedia\n",
      "Test text 2\n",
      "http://www.quackit.com/html/tutorial/html_links.cfm,Example Link\n",
      "http://www.quackit.com/html/examples/html_links_examples.cfm,More Link Examples...\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"\"\"\n",
    "13\n",
    "<div class=\"portal\" role=\"navigation\" id='p-navigation'>\n",
    "<h3>Navigation</h3>\n",
    "<div class=\"body\">\n",
    "<ul>\n",
    " <li id=\"n-mainpage-description\"><a href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\" accesskey=\"z\">Main page</a></li>\n",
    " <li id=\"n-contents\"><a href=\"/wiki/Portal:Contents\" title=\"Guides to browsing Wikipedia\">Contents</a></li>\n",
    " <li id=\"n-featuredcontent\"><a href=\"/wiki/Portal:Featured_content\" title=\"Featured content  the best of Wikipedia\">Featured content</a></li>\n",
    "<li id=\"n-currentevents\"><a href=\"/wiki/Portal:Current_events\" title=\"Find background information on current events\">Current events</a></li>\n",
    "<li id=\"n-randompage\"><a href=\"/wiki/Special:Random\" title=\"Load a random article [x]\" accesskey=\"x\">Random article</a></li>\n",
    "<li id=\"n-sitesupport\"><a href=\"//donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\" title=\"Support us\">Donate to Wikipedia</a></li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>  \n",
    "\"\"\",\n",
    "\"\"\"\n",
    "2\n",
    "<p><a href=\"http://www.quackit.com/html/tutorial/html_links.cfm\">Example Link</a></p>\n",
    "<div class=\"more-info\"><a href=\"http://www.quackit.com/html/examples/html_links_examples.cfm\">More Link Examples...</a></div>\n",
    "\"\"\"         ]\n",
    "for i, test_text in enumerate(test_texts):\n",
    "    print(f'Test text {i+1}')\n",
    "    for el in search_links(test_text):\n",
    "        print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxic\n",
       "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0\n",
       "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0\n",
       "2                          Собаке - собачья смерть\\n    1.0\n",
       "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0\n",
       "4  тебя не убедил 6-страничный пдф в том, что Скр...    1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stemmer = PorterStemmer()\n",
    "del_words = stopwords.words('russian')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "CountVec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    sub_text = re.sub('[^А-Яа-яA-Za-z\\s\\|]|\\s{2,}',' ',string).strip().lower()\n",
    "    tokens = word_tokenize(str(sub_text))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(words_list):\n",
    "    stemmed_words = [stemmer.stem(word) for word in words_list if word not in del_words]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(words_list):\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words_list if word not in del_words]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(sentences_list):\n",
    "    Count_data = CountVec.fit_transform(sentences_list)\n",
    "    return Count_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_form(words_list):\n",
    "    normal_form_tokens = [morph.parse(word)[0].normal_form for word in words_list if word not in del_words]\n",
    "    return normal_form_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['каких', 'стан', 'является', 'эталоном', 'современная', 'система', 'здравоохранения', 'рф', 'зимбабве', 'тупой', 'хохлы']\n",
      "['каких', 'стан', 'является', 'эталоном', 'современная', 'система', 'здравоохранения', 'рф', 'зимбабве', 'тупой', 'хохлы']\n"
     ]
    }
   ],
   "source": [
    "test_string = df['comment'][5]\n",
    "test_stemming = stemming(clean_text(test_string))\n",
    "print(test_stemming)\n",
    "test_lemmatizer = lemmatizing(clean_text(test_string))\n",
    "print(test_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['comment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences = [' '.join( normal_form(clean_text(sentence)) ) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['верблюдов дебил бл',\n",
       " 'хохол это отдушина затюканый россиянин мол вон хохлов худой хохлов кисель придумать',\n",
       " 'собака собачий смерть',\n",
       " 'страница обновить дебил это оскорбление доказать факт дебил множественный число писать верить это твой воображать друг',\n",
       " 'убедить страничный пдф скрипаль отравить россия анализировать думать пытаться ватник',\n",
       " 'какой стан являться эталон современный система здравоохранение рф зимбабве тупой хохол',\n",
       " 'шапка ссылка инф текущий фильм марвести ссылка заменить фраза репортить брипидора игнорировать пост недостаточно понять модератор абсолютный неадекват нужно лишить полномочие этот борд пробивать абсолютный дно неадекватность',\n",
       " 'упад том строить технология разворовать трещина пош литр тупой китаз мочь нормальный сделать',\n",
       " 'ебать разносить шизик',\n",
       " 'обосраться сидеть обтекать']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = bow(clean_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(res[:1000].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dataframe=pd.DataFrame(res[:1000].toarray(),columns=CountVec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abaqus</th>\n",
       "      <th>ac</th>\n",
       "      <th>academy</th>\n",
       "      <th>accaмблея</th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>accountability</th>\n",
       "      <th>accounts</th>\n",
       "      <th>accwiz</th>\n",
       "      <th>acronis</th>\n",
       "      <th>...</th>\n",
       "      <th>яя</th>\n",
       "      <th>яяяясный</th>\n",
       "      <th>ёвр</th>\n",
       "      <th>ёж</th>\n",
       "      <th>ёлка</th>\n",
       "      <th>ёлочка</th>\n",
       "      <th>ёмкий</th>\n",
       "      <th>ёмкость</th>\n",
       "      <th>ёрзать</th>\n",
       "      <th>ёрничать</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34236 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abaqus  ac  academy  accaмблея  access  account  accountability  accounts  \\\n",
       "0       0   0        0          0       0        0               0         0   \n",
       "1       0   0        0          0       0        0               0         0   \n",
       "2       0   0        0          0       0        0               0         0   \n",
       "3       0   0        0          0       0        0               0         0   \n",
       "4       0   0        0          0       0        0               0         0   \n",
       "\n",
       "   accwiz  acronis  ...  яя  яяяясный  ёвр  ёж  ёлка  ёлочка  ёмкий  ёмкость  \\\n",
       "0       0        0  ...   0         0    0   0     0       0      0        0   \n",
       "1       0        0  ...   0         0    0   0     0       0      0        0   \n",
       "2       0        0  ...   0         0    0   0     0       0      0        0   \n",
       "3       0        0  ...   0         0    0   0     0       0      0        0   \n",
       "4       0        0  ...   0         0    0   0     0       0      0        0   \n",
       "\n",
       "   ёрзать  ёрничать  \n",
       "0       0         0  \n",
       "1       0         0  \n",
       "2       0         0  \n",
       "3       0         0  \n",
       "4       0         0  \n",
       "\n",
       "[5 rows x 34236 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP1_homework",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
